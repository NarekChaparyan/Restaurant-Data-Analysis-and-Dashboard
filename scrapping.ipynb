{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b910da4-a6c8-43ff-9ded-0ec84c239fa2",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cea94-a5f9-4387-8adc-47b27aca3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddda713-9b2e-4f0d-8d2f-340401be7c89",
   "metadata": {},
   "source": [
    "# 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6e28a-c7aa-4ca0-987e-56dee9b17936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_restaurant(driver, url):\n",
    "    global rest_count\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        rest_name = driver.find_element(By.XPATH, '//h1').text\n",
    "    except NoSuchElementException:\n",
    "        rest_name = \"Name not found\"\n",
    "    \n",
    "    try:\n",
    "        number_of_reviews = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/span[2]').text\n",
    "    except NoSuchElementException:\n",
    "        number_of_reviews = \"Reviews not found\"\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element(By.XPATH, './/span[contains(@class, \"m1KNa9XKCHY- C7Tp-bANpE4-\")]').text\n",
    "    except NoSuchElementException:\n",
    "        rating = \"Rating not found\"\n",
    "    \n",
    "    try:\n",
    "        food_type = driver.find_element(By.XPATH, '//*[@id=\"cuisineInfo\"]/span[2]').text\n",
    "    except NoSuchElementException:\n",
    "        food_type = \"Food type not found\"\n",
    "    \n",
    "    try:\n",
    "        coupon = driver.find_element(By.XPATH, '//div[contains(@id, \"priceBandInfo\")]//span[last()]').text\n",
    "    except NoSuchElementException:\n",
    "        coupon = \"Coupon not found\"\n",
    "\n",
    "    try:\n",
    "        food = driver.find_element(By.XPATH, '//span[text()=\"Food\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        food = \"Food not found\"\n",
    "\n",
    "    try:\n",
    "        service = driver.find_element(By.XPATH, '//span[text()=\"Service\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        service = \"Service not found\"\n",
    "\n",
    "    try:\n",
    "        ambience = driver.find_element(By.XPATH, '//span[text()=\"Ambience\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        ambience = \"Ambience not found\"\n",
    "\n",
    "    try:\n",
    "        value = driver.find_element(By.XPATH, '//span[text()=\"Value\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        value = \"Value not found\"\n",
    "        \n",
    "    try:\n",
    "        image_elements = driver.find_elements(By.XPATH, '//img[contains(@src, \"otstatic.com\")]')\n",
    "        if len(image_elements) > 1:\n",
    "            second_image_url = image_elements[1].get_attribute('src')\n",
    "        else:\n",
    "            second_image_url = \"No valid image found\"\n",
    "    except NoSuchElementException:\n",
    "        second_image_url = \"No image found\"\n",
    "    \n",
    "    all_comments = []\n",
    "    max_pages = 3\n",
    "    current_page = 1\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        try:\n",
    "            comments_elements = driver.find_elements(By.XPATH, './/span[contains(@class, \"l9bbXUdC9v0- ZatlKKd1hyc- ukvN6yaH1Ds-\")]')\n",
    "            comments = \" \".join([element.text for element in comments_elements[1:]]) if comments_elements else \"Comments not found\"\n",
    "            all_comments.append(comments)\n",
    "            print(f\"Page {current_page} Comments:\", comments)\n",
    "            \n",
    "            try:\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@aria-label=\"Go to the next page\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(2)\n",
    "                current_page += 1\n",
    "            except (TimeoutException, ElementNotInteractableException, StaleElementReferenceException):\n",
    "                print(\"Failed to locate or click the next button. Ending pagination.\")\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    \n",
    "    all_comments = \" \".join(all_comments)\n",
    "\n",
    "    try:\n",
    "        about_rest = driver.find_element(By.XPATH, './/span[contains(@class, \"l9bbXUdC9v0- ZatlKKd1hyc- ukvN6yaH1Ds- l-AMWW5ZrIg-\")]').text\n",
    "    except NoSuchElementException:\n",
    "        about_rest = \"About Restaurant not found\"\n",
    "    print(\"About Restaurant:\", about_rest)\n",
    "\n",
    "    data[\"url\"].append(url)\n",
    "    data[\"rest_name\"].append(rest_name)\n",
    "    data[\"number_of_reviews\"].append(number_of_reviews)\n",
    "    data[\"rating\"].append(rating)\n",
    "    data[\"food_type\"].append(food_type)\n",
    "    data[\"coupon\"].append(coupon)\n",
    "    data[\"food\"].append(food)\n",
    "    data[\"service\"].append(service)\n",
    "    data[\"ambience\"].append(ambience)\n",
    "    data[\"value\"].append(value)\n",
    "    data[\"about_rest\"].append(about_rest)\n",
    "    data[\"comments\"].append(all_comments)\n",
    "    data[\"image_url\"].append(second_image_url)\n",
    "\n",
    "    rest_count += 1\n",
    "    print(f\"Total Restaurants scraped: {rest_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee02cd-5479-4482-9746-bffa9c5f8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_links(driver):\n",
    "    all_urls = []\n",
    "    time.sleep(random.randint(2, 3))\n",
    "\n",
    "    scroll_increment = 500\n",
    "\n",
    "    for _ in range(22):\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"//a[contains(@class, 'qCITanV81-Y-')]\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for restaurant links to become visible\")\n",
    "            break\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, \"//a[contains(@class, 'qCITanV81-Y-')]\")\n",
    "\n",
    "        for elem in elements:\n",
    "            url = elem.get_attribute('href')\n",
    "            if url and url not in all_urls:\n",
    "                all_urls.append(url)\n",
    "        \n",
    "    print(f\"Total URLs collected so far: {len(all_urls)}\")\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "def click_next_page(driver):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@aria-label='Go to the next page']\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Go to the next page']\")))\n",
    "        next_button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException, TimeoutException) as e:\n",
    "        print(f\"Could not click next page: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda2f76-4702-4dc6-9f31-fe9a1bcf4590",
   "metadata": {},
   "source": [
    "# 3. Initialize Data Structure and Set Up Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae680b6-85d6-4eb2-b0d8-4974131b63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"url\": [],\n",
    "    \"rest_name\": [],\n",
    "    \"number_of_reviews\": [],\n",
    "    \"rating\": [],\n",
    "    \"food_type\": [],\n",
    "    \"coupon\": [],\n",
    "    \"food\": [],\n",
    "    \"service\": [],\n",
    "    \"ambience\": [],\n",
    "    \"value\": [],\n",
    "    \"about_rest\": [],\n",
    "    \"comments\": [],\n",
    "    \"image_url\": []\n",
    "}\n",
    "\n",
    "search_url = 'https://www.opentable.com/s?dateTime=2024-11-10T19%3A00%3A00&covers=4&latitude=53.403769&longitude=-2.983646&metroId=&originalTerm=liverpool&intentModifiedTerm=&areaId=geohash%3Agcmzu1g7&suggestedSearchName=Liverpool&shouldUseLatLongSearch=true&originCorrelationId=9f79d4e1-985b-44bc-922d-0b240c89dbe9'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(search_url)\n",
    "\n",
    "all_urls = []\n",
    "rest_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b2db1-c75f-4b05-ae5d-b6780cfaf0eb",
   "metadata": {},
   "source": [
    "# 4. Scrape Restaurant URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd3eae-b633-4bc3-b428-5bc8d38e7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    urls = get_restaurant_links(driver)\n",
    "    all_urls.extend(urls)\n",
    "\n",
    "    if not click_next_page(driver):\n",
    "        print(\"No more pages to navigate.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682f286-6d8a-4703-97c8-442256f5a9b4",
   "metadata": {},
   "source": [
    "# 5. Scrape Data for Each Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824cb82-fa78-4edc-b670-d9ea07768186",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in all_urls:\n",
    "    scrape_restaurant(driver, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f61265-4c8b-40ba-8086-ec15e56a8b6d",
   "metadata": {},
   "source": [
    "# 6. Save Data and Close the Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d06d-5764-44f8-a8eb-07221e621430",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('Liverpool.csv', index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258b85a-d5c5-4ff8-9c2d-b52b2903df62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
